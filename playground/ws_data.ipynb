{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from itertools import product\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional, Dict, Any, Union, Callable, Iterable"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df14373f",
   "metadata": {},
   "source": [
    "from scaling.utils import (\n",
    "    get_pareto_frontier, \n",
    "    get_final_points_from_curve_set, \n",
    "    fit_linear_model,\n",
    "    functional_form_chin3,\n",
    "    fit_parametric_form,\n",
    "    fit_parametric_form_stable,\n",
    "    functional_form_chin3_stable,\n",
    ")\n",
    "from scaling.visualize import visualize_train_curves, plot_line_fit, plot_isoflops"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac78824fe6083e61",
   "metadata": {},
   "source": [
    "unique_col_list = [\"base_N\", \"target_N\", \"tkpm\", \"shrink\"]\n",
    "\n",
    "def preprocess_warmstarting(df, y_col_to_smooth=None, smoothing_window=100):\n",
    "    __df = pd.DataFrame()\n",
    "    for i, x in enumerate(df.groupby(unique_col_list)):\n",
    "        _df = x[1].sort_values(by=\"flops\")\n",
    "        # smooth it\n",
    "        if y_col_to_smooth is not None:\n",
    "            # +\"_smoothed\"\n",
    "            _df[y_col_to_smooth] = _df[y_col_to_smooth].rolling(smoothing_window, win_type='gaussian', min_periods=1).mean(std=smoothing_window / 10)\n",
    "        \n",
    "        # scaling tokens and flops to the max\n",
    "        max_intended_tokens = (_df.iloc[-1][\"target_N\"] * _df.iloc[-1][\"tkpm\"])\n",
    "        if abs((max_intended_tokens -  _df[\"tokens\"].max()) / _df[\"tokens\"].max()) > 0.01:\n",
    "            print(\"Wrong tkpm: \", x[0])\n",
    "            continue\n",
    "        _df[\"tokens\"] = np.round(max_intended_tokens / _df[\"tokens\"].max() * _df[\"tokens\"])\n",
    "        \n",
    "        max_intended_flops = 6. * max_intended_tokens * _df[\"target_N\"]\n",
    "        _df[\"flops\"] = np.round(max_intended_flops / _df[\"flops\"].max() * _df[\"flops\"])\n",
    "        \n",
    "        __df = pd.concat([__df, _df])\n",
    "    return __df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1395b0c2d7734b3",
   "metadata": {},
   "source": [
    "warmstarting_df = pd.read_parquet(\n",
    "    \"../data/warmstarting_results.parquet\",\n",
    ")\n",
    "warmstarting_df = preprocess_warmstarting(warmstarting_df)\n",
    "display(warmstarting_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dceef6a3",
   "metadata": {},
   "source": [
    "# retaining only warmstarting results\n",
    "\n",
    "warmstarting_df = warmstarting_df.loc[warmstarting_df.method != \"mup\"]\n",
    "display(warmstarting_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a452e60b",
   "metadata": {},
   "source": [
    "### Visualize raw data\n",
    "\n",
    "Visualize all learning curves across N, D available in the *training set* to see scaling patterns.\n",
    "\n",
    "Optionally, consider visualizing for different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "3950184d",
   "metadata": {},
   "source": [
    "\n",
    "y_col = \"Validation Loss\"\n",
    "x_col = \"flops\"  # \"tokens\"\n",
    "\n",
    "final_points = get_final_points_from_curve_set(\n",
    "    warmstarting_df,\n",
    "    unique_col_list,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    get_pareto=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7e24e02",
   "metadata": {},
   "source": [
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "pareto_df = visualize_train_curves(\n",
    "    ax, \n",
    "    warmstarting_df,\n",
    "    unique_col_list,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    plot_all_curves=True,\n",
    "    plot_final=True,\n",
    "    plot_pareto_final=True,\n",
    "    ylims=(1.5, 2.5),\n",
    "    xlims=(1e16, 2e19),\n",
    "    xlog=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ffba11f5-a6f1-4121-9713-dd40fd82c8a5",
   "metadata": {},
   "source": [
    "display(pareto_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "336e3a14",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "\n",
    "Fit for `C vs N` and `C vs D`, assuming *best* training run for each `(N, D)`.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Visualize for `C vs N` and `C vs D`\n",
    "2. Fit linear model for each of them"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a1206df",
   "metadata": {},
   "source": [
    "x_col = \"flops\"\n",
    "y_col = \"Validation Loss\"\n",
    "\n",
    "_pareto_df = get_final_points_from_curve_set(\n",
    "    warmstarting_df,\n",
    "    unique_col_list,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    get_pareto=True,\n",
    ")\n",
    "\n",
    "display(_pareto_df.head())\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[0],\n",
    "    X = _pareto_df[x_col],\n",
    "    Y = _pareto_df[\"target_N\"]\n",
    ")\n",
    "ax[0].set_ylabel(\"Target N\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[1],\n",
    "    X = _pareto_df[x_col],\n",
    "    Y = _pareto_df[\"tokens\"],\n",
    ")\n",
    "ax[1].set_ylabel(\"Tokens\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[2],\n",
    "    X = _pareto_df[x_col],\n",
    "    Y = _pareto_df[y_col],\n",
    ")\n",
    "ax[2].set_ylabel(y_col)\n",
    "\n",
    "fig.supxlabel(\"FLOPs\")\n",
    "\n",
    "for ax in ax.flat:\n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        leg.remove()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e92aac0",
   "metadata": {},
   "source": [
    "##### Using the largest model scale as the held out"
   ]
  },
  {
   "cell_type": "code",
   "id": "55ad2df6",
   "metadata": {},
   "source": [
    "# Creating held out over pareto df HERE\n",
    "\n",
    "train_df = _pareto_df.loc[_pareto_df.target_N != sorted(_pareto_df.target_N.unique())[-1]]\n",
    "held_out_df = _pareto_df.loc[_pareto_df.target_N == sorted(_pareto_df.target_N.unique())[-1]]\n",
    "held_out_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd51a93e",
   "metadata": {},
   "source": [
    "# Visualizing scaling law linear fit for C vs N, D, L\n",
    "\n",
    "plt.clf();\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4.5));\n",
    "\n",
    "# C vs N\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[\"target_N\"].values\n",
    "slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[0],\n",
    "    X,\n",
    "    Y,\n",
    "    slope,\n",
    "    intercept,\n",
    "    x_extrapolate=[held_out_df.flops.values[0], 1e20, 1e21, 1e22, 1e23],\n",
    "    y_extrapolate=[held_out_df[\"target_N\"].values[0]],\n",
    ")\n",
    "ax[0].set_ylabel(\"Target N\")\n",
    "\n",
    "\n",
    "# C vs D\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[\"tokens\"].values\n",
    "slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[1],\n",
    "    X,\n",
    "    Y,\n",
    "    slope,\n",
    "    intercept,\n",
    "    x_extrapolate=[held_out_df.flops.values[0], 1e20, 1e21, 1e22, 1e23],\n",
    "    y_extrapolate=[held_out_df.tokens.values[0]],\n",
    ")\n",
    "ax[1].set_ylabel(\"Tokens\")\n",
    "\n",
    "# C vs Loss\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[y_col].values\n",
    "slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[2],\n",
    "    X,\n",
    "    Y,\n",
    "    slope,\n",
    "    intercept,\n",
    "    x_extrapolate=[held_out_df.flops.values[0], 1e20, 1e21, 1e22, 1e23],\n",
    "    y_extrapolate=[held_out_df[y_col].values[0]],\n",
    ")\n",
    "ax[2].set_ylabel(y_col)\n",
    "\n",
    "fig.supxlabel(\"FLOPs\")\n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "for ax in ax.flat:\n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        leg.remove()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81a57d3f",
   "metadata": {},
   "source": [
    "##### Using the top-2 model scales as the held out"
   ]
  },
  {
   "cell_type": "code",
   "id": "5769f0dc",
   "metadata": {},
   "source": [
    "# Creating held out over pareto df HERE\n",
    "\n",
    "train_df = _pareto_df.loc[_pareto_df.target_N.isin(sorted(_pareto_df.target_N.unique())[:-2])]\n",
    "held_out_df = _pareto_df.loc[_pareto_df.target_N.isin(sorted(_pareto_df.target_N.unique())[-2:])]\n",
    "held_out_df = held_out_df.loc[held_out_df.tkpm.isin(train_df.tkpm.unique())]\n",
    "held_out_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9c8448a",
   "metadata": {},
   "source": [
    "# Visualizing scaling law linear fit for C vs N, D, L\n",
    "\n",
    "plt.clf();\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4.5));\n",
    "\n",
    "# C vs N\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[\"target_N\"].values\n",
    "slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[0],\n",
    "    X,\n",
    "    Y,\n",
    "    slope,\n",
    "    intercept,\n",
    "    x_extrapolate=held_out_df.flops.values.tolist() + [1e20, 1e21, 1e22, 1e23],\n",
    "    y_extrapolate=held_out_df[\"target_N\"].values,\n",
    ")\n",
    "ax[0].set_ylabel(\"Target N\")\n",
    "\n",
    "\n",
    "# C vs D\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[\"tokens\"].values\n",
    "slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[1],\n",
    "    X,\n",
    "    Y,\n",
    "    slope,\n",
    "    intercept,\n",
    "    x_extrapolate=held_out_df.flops.values.tolist() + [1e20, 1e21, 1e22, 1e23],\n",
    "    y_extrapolate=held_out_df.tokens.values,\n",
    ")\n",
    "ax[1].set_ylabel(\"Tokens\")\n",
    "\n",
    "# C vs Loss\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[y_col].values\n",
    "slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "\n",
    "plot_line_fit(\n",
    "    ax[2],\n",
    "    X,\n",
    "    Y,\n",
    "    slope,\n",
    "    intercept,\n",
    "    x_extrapolate=held_out_df.flops.values.tolist() + [1e20, 1e21, 1e22, 1e23],\n",
    "    y_extrapolate=held_out_df[y_col].values,\n",
    ")\n",
    "ax[2].set_ylabel(y_col)\n",
    "\n",
    "fig.supxlabel(\"FLOPs\")\n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "for ax in ax.flat:\n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        leg.remove()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "947958b5-3f12-4488-8d79-60154f677686",
   "metadata": {},
   "source": [
    "#### Fixed Base Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "d94d5f9d-12c0-4a44-ad1b-65df03365fbc",
   "metadata": {},
   "source": [
    "# Visualize the Pareto Front and display to what algorithm it belongs (I think we just have muP and Paws...)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c54b1aa-b65f-4b77-ad86-535f543c4dbb",
   "metadata": {},
   "source": [
    "warmstarting_df = pd.read_parquet(\n",
    "    \"../data/warmstarting_results.parquet\",\n",
    ")\n",
    "warmstarting_df = preprocess_warmstarting(warmstarting_df)\n",
    "base_model_sizes = warmstarting_df['base_N'].unique()\n",
    "\n",
    "# create a plot with len(basemodel sizes) many axes\n",
    "fig, axes = plt.subplots(1, len(base_model_sizes), figsize=(5 * len(base_model_sizes), 4.5));\n",
    "fig2, axes2 = plt.subplots(1, len(base_model_sizes), figsize=(5 * len(base_model_sizes), 4.5));\n",
    "\n",
    "for i, base_model_size in enumerate(base_model_sizes):\n",
    "    base_size_df = warmstarting_df[warmstarting_df['base_N'] == base_model_size]\n",
    "    \n",
    "    axes[i].set_title(f\"Base model size: {(base_model_size/1_000_000):3.1f}M\")\n",
    "    # plot for the different scaling factors \n",
    "    # get a color gradient\n",
    "\n",
    "    shrink_factors = sorted(base_size_df['shrink'].unique())\n",
    "    colors = plt.cm.Blues(np.linspace(0, 1, len(shrink_factors)))\n",
    "    for j, shrink in enumerate(shrink_factors):\n",
    "        shrink_df = base_size_df[base_size_df['shrink']==shrink]\n",
    "        if shrink in [0.0, 0.2]:\n",
    "            visualize_train_curves(\n",
    "                axes[i], \n",
    "                shrink_df,\n",
    "                unique_col_list,\n",
    "                x_col=x_col,\n",
    "                y_col=y_col,\n",
    "                plot_all_curves=True,\n",
    "                plot_final=True,\n",
    "                plot_pareto_final=True,\n",
    "                ylims=(1.5, 2.5),\n",
    "                xlims=(1e16, 2e19),\n",
    "                xlog=True,\n",
    "                style={\"color\": plt.get_cmap(\"tab10\").colors[j], \"label\": f\"shrink={shrink}\"})\n",
    "\n",
    "        pareto_shrink_df = get_final_points_from_curve_set(\n",
    "            shrink_df,\n",
    "            unique_col_list,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            get_pareto=True,\n",
    "        )\n",
    "            \n",
    "        X = pareto_shrink_df[x_col].values\n",
    "        Y = pareto_shrink_df[y_col].values\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "        # print(f\"Slope: {slope}, Intercept: {intercept}, R^2: {r_value**2}\")\n",
    "        if shrink == 0.:\n",
    "            plot_line_fit(\n",
    "                axes2[i],\n",
    "                X,\n",
    "                Y,\n",
    "                slope,\n",
    "                intercept,\n",
    "                style={\"color\": \"red\", \"label\": f\"shrink={shrink}\"}\n",
    "            )\n",
    "        else:\n",
    "            plot_line_fit(\n",
    "                axes2[i],\n",
    "                X,\n",
    "                Y,\n",
    "                slope,\n",
    "                intercept,\n",
    "                style={\"color\": colors[j], \"label\": f\"shrink={shrink}\"})\n",
    "    # axes2[-1].legend()\n",
    "    # list all scaling factors\n",
    "    # Add the line plot in there\n",
    "    # try to somehow add labels and colors\n",
    "    \n",
    "    # Possibly assign colors based of identifier columns\n",
    "    # Fit the loss for each shrinking?\n",
    "    # Fit the loss for all approaches \n",
    "for ax in axes2.flat:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        pass # leg.remove()\n",
    "\n",
    "for ax in axes.flat:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        pass # leg.remove()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4f6b32b-a329-4ba1-991a-1d929d913451",
   "metadata": {},
   "source": [
    "## Fixed Growth Factor"
   ]
  },
  {
   "cell_type": "code",
   "id": "a74a2073-3e27-471e-a9d1-ac653faaaa13",
   "metadata": {},
   "source": [
    "def filter_pairs(df, jump_size=1):\n",
    "    distinct_values = sorted(\n",
    "        pd.unique(df[['base_N', 'target_N']].values.ravel())\n",
    "    )\n",
    "    pairs = list(zip(distinct_values[:-jump_size], distinct_values[jump_size:]))\n",
    "    df_filtered = df[df[['base_N', 'target_N']].apply(tuple, axis=1).isin(pairs)]\n",
    "    return df_filtered\n",
    "\n",
    "y_col = \"Validation Loss\"\n",
    "\n",
    "num_jump_sizes = 1\n",
    "fig, axes = plt.subplots(1, num_jump_sizes, figsize=((5 * num_jump_sizes), 4.5), layout='constrained');\n",
    "axes = np.atleast_1d(axes)\n",
    "fig2, axes2 = plt.subplots(1, num_jump_sizes, figsize=((5 * num_jump_sizes), 4.5), layout='constrained');\n",
    "axes2 = np.atleast_1d(axes2)\n",
    "\"\"\"\n",
    "mup_df = warmstarting_df[warmstarting_df['method']=='mup']\n",
    "pareto_shrink_df = get_final_points_from_curve_set(\n",
    "    mup_df,\n",
    "    unique_col_list,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    get_pareto=True,\n",
    ")\n",
    "X = pareto_shrink_df[x_col].values\n",
    "Y = pareto_shrink_df[y_col].values\n",
    "for i in range(num_jump_sizes):\n",
    "    plot_line_fit(\n",
    "        axes[i],\n",
    "        X,\n",
    "        Y,\n",
    "        slope,\n",
    "        intercept,\n",
    "        style={\"color\": \"red\", \"label\": f\"mup\"}\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "for i in range(num_jump_sizes):\n",
    "\n",
    "    axes[i].set_title(f\"{(2)**(i+1)}x growth factor\")\n",
    "    jump_df = filter_pairs(warmstarting_df, jump_size=i+1)\n",
    "\n",
    "    shrink_factors = sorted(jump_df['shrink'].unique())\n",
    "    colors = plt.cm.Blues(np.linspace(0, 1, len(shrink_factors)))\n",
    "    for j, shrink in enumerate(shrink_factors):\n",
    "        if shrink not in [0.0, 1.0, 0.4]:\n",
    "            continue\n",
    "        shrink_df = jump_df[jump_df['shrink']==shrink]\n",
    "        pareto_shrink_df = get_final_points_from_curve_set(\n",
    "            shrink_df,\n",
    "            unique_col_list,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            get_pareto=True,\n",
    "        )\n",
    "            \n",
    "        X = pareto_shrink_df[x_col].values\n",
    "        Y = pareto_shrink_df[y_col].values\n",
    "\n",
    "        if shrink in [0.0, 1.0, 0.4]:\n",
    "            visualize_train_curves(\n",
    "                axes2[i], \n",
    "                shrink_df,\n",
    "                unique_col_list,\n",
    "                x_col=x_col,\n",
    "                y_col=y_col,\n",
    "                plot_all_curves=True,\n",
    "                plot_final=True,\n",
    "                plot_pareto_final=True,\n",
    "                ylims=(1.5, 2.5),\n",
    "                xlims=(1e16, 2e19),\n",
    "                xlog=True,\n",
    "                style={\"color\": plt.get_cmap(\"tab10\").colors[j], \"label\": f\"shrink={shrink}\"})\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = fit_linear_model(X, Y)\n",
    "        \n",
    "        if shrink != 0.0:\n",
    "            style={\"color\": colors[j], \"label\": r\"$\\lambda_{shrink}\" + rf\"={shrink}$ ($L={intercept:.2f}\\cdot C^\" + r\"{\" f\"{slope:.3f}\" + r\"}$)\"}\n",
    "        else:\n",
    "            style={\"color\": \"red\", \"label\": rf\"$\\mu P$ ($L={intercept:.2f}\\cdot C^\" + r\"{\" f\"{slope:.3f}\" + r\"}$)\"}\n",
    "        \n",
    "        plot_line_fit(\n",
    "            axes[i],\n",
    "            X,\n",
    "            Y,\n",
    "            slope,\n",
    "            intercept,\n",
    "            style=style,\n",
    "            x_max_plot=1e19\n",
    "        )\n",
    "\n",
    "fig.supxlabel(\"FLOPs\")\n",
    "fig.supylabel(y_col)\n",
    "for ax in axes.flat:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        pass # leg.remove()\n",
    "\n",
    "for ax in axes2.flat:\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    leg = ax.get_legend()\n",
    "    if leg:\n",
    "        pass # leg.remove()\n",
    "\n",
    "# set axis to linear scale\n",
    "for ax in axes.flat:\n",
    "    ax.set_xscale('linear') \n",
    "    ax.set_yscale('linear') \n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4403b133",
   "metadata": {},
   "source": [
    "### Approach 3\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "588e2e64",
   "metadata": {},
   "source": [
    "mup_df = warmstarting_df[warmstarting_df['method']=='mup']\n",
    "df = get_final_points_from_curve_set(\n",
    "    mup_df,\n",
    "    unique_col_list,\n",
    "    x_col=\"flops\",\n",
    "    y_col=\"Validation Loss\",\n",
    "    get_pareto=False,\n",
    ")\n",
    "\n",
    "N, _ = df[\"target_N\"].values, df[\"tokens\"].values\n",
    "_D = df[\"target_N\"] * df[\"tkpm\"]\n",
    "y = df[\"Validation Loss\"].values\n",
    "\n",
    "_df = pd.DataFrame.from_dict({\n",
    "    \"N\": N,\n",
    "    \"D\": _D,\n",
    "    \"Loss\": y\n",
    "}).groupby(by=[\"N\", \"D\"]).min().reset_index()\n",
    "_df.sort_values(by=[\"N\", \"D\"], inplace=True)\n",
    "\n",
    "data_X = _df[[\"N\", \"D\"]].values\n",
    "data_y = _df[\"Loss\"].values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59c03807",
   "metadata": {},
   "source": [
    "initialization = list(product(\n",
    "    np.linspace(0, 15, 5),  # a\n",
    "    np.linspace(0., 1., 10),  # alpha\n",
    "    np.linspace(0, 15, 5),  # b\n",
    "    np.linspace(0., 1., 10),  # beta\n",
    "    np.linspace(-1., 1., 5),  # e\n",
    "))\n",
    "\n",
    "# initialization = list(product(\n",
    "#     [0, 5,],  # a\n",
    "#     [0, 0.5,],  # alpha\n",
    "#     [0, 5, ],  # b\n",
    "#     [0, 0.5,],  # beta\n",
    "#     [-1, -0.5,]  # e\n",
    "# ))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79854842",
   "metadata": {},
   "source": [
    "best_params, best_loss = fit_parametric_form_stable(\n",
    "    functional_form_chin3_stable,\n",
    "    data_X, \n",
    "    data_y, \n",
    "    initialization\n",
    ")\n",
    "\n",
    "print(f\"Best Loss: {best_loss}\")\n",
    "print(f\"a: {best_params[0]}, alpha={best_params[1]}\\nb: {best_params[2]}, beta={best_params[3]}\\ne={best_params[4]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79f534dd",
   "metadata": {},
   "source": [
    "_a, alpha, _b, beta, _e = best_params\n",
    "\n",
    "A = np.exp(_a)\n",
    "B = np.exp(_b)\n",
    "E = np.exp(_e)\n",
    "\n",
    "a = beta / (alpha + beta)\n",
    "b = alpha / (alpha + beta)\n",
    "\n",
    "print(a, b)\n",
    "\n",
    "G = ((alpha*A) / (beta*B)) ** (1 / (alpha + beta))\n",
    "print(G)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15d1e5a4",
   "metadata": {},
   "source": [
    "# Creating held out over pareto df HERE\n",
    "x_col = \"flops\"\n",
    "y_col = \"Validation Loss\"\n",
    "\n",
    "_pareto_df = get_final_points_from_curve_set(\n",
    "    mup_df,\n",
    "    unique_col_list,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    get_pareto=True\n",
    ")\n",
    "train_df = _pareto_df\n",
    "# train_df = _pareto_df.loc[_pareto_df.target_N != sorted(_pareto_df.target_N.unique())[-1]]\n",
    "# held_out_df = _pareto_df.loc[_pareto_df.target_N == sorted(_pareto_df.target_N.unique())[-1]]\n",
    "# held_out_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8c54e8f",
   "metadata": {},
   "source": [
    "# plotting for N_opt\n",
    "\n",
    "plt.clf();\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4));\n",
    "\n",
    "# C vs N\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[\"target_N\"].values\n",
    "# slope, intercept = b, G / 6**a\n",
    "\n",
    "ax[0].scatter(X, Y, label=\"raw data\")\n",
    "_x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "ax[0].plot(\n",
    "    _x_plot, \n",
    "    # np.exp(intercept + slope * np.log(_x_plot)), \n",
    "    G * (_x_plot / 6) ** a,\n",
    "    color=\"red\", \n",
    "    label=\"fitted line\"\n",
    "    )\n",
    "ax[0].loglog()\n",
    "\n",
    "# C vs D\n",
    "\n",
    "X = train_df[x_col].values\n",
    "Y = train_df[\"tokens\"].values\n",
    "# slope, intercept = b, G / 6**a\n",
    "\n",
    "ax[1].scatter(X, Y, label=\"raw data\")\n",
    "_x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "ax[1].plot(\n",
    "    _x_plot, \n",
    "    # np.exp(intercept + slope * np.log(_x_plot)), \n",
    "    G**-1 * (_x_plot / 6) ** b,\n",
    "    color=\"red\", \n",
    "    label=\"fitted line\"\n",
    "    )\n",
    "ax[1].loglog()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be5c47a55af5904f",
   "metadata": {},
   "source": [
    "# Isoflops"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b8538c20488503",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "def get_loss_at_flops(df: pd.DataFrame, flop_intervals: list[float], y_col: str, unique_col_list = list[str], add_base_compute=False) -> pd.Series:\n",
    "    \"\"\"Get the loss at a specific flop value by interpolation.\"\"\"\n",
    "    x_col = \"flops\"\n",
    "    best_learning_curve = None\n",
    "    best_final_loss = float('inf')\n",
    "\n",
    "    for i, x in enumerate(df.groupby(unique_col_list)):\n",
    "        _df = x[1].dropna(subset=[y_col]).sort_values(by=x_col)\n",
    "        if add_base_compute:\n",
    "            base_flops = 6. * 20. * _df.iloc[0]['base_N']**2\n",
    "            _df[x_col] += base_flops\n",
    "        final_loss = _df.iloc[-1][y_col]\n",
    "        if final_loss < best_final_loss:\n",
    "            best_final_loss = final_loss\n",
    "            best_learning_curve = pd.Series(\n",
    "                data=_df[y_col].values,\n",
    "                index=_df[x_col].values\n",
    "            )\n",
    "    \n",
    "    # add the flops into the Series if not present\n",
    "    for flop in flop_intervals:\n",
    "        if flop not in best_learning_curve.index:\n",
    "            best_learning_curve.loc[flop] = np.nan\n",
    "    best_learning_curve = best_learning_curve.sort_index()\n",
    "    # interpolate nans\n",
    "    best_learning_curve = best_learning_curve.interpolate(method='linear')\n",
    "    return best_learning_curve.loc[flop_intervals]\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1abf9c64f4796026",
   "metadata": {},
   "source": [
    "SHRINK = 0.4\n",
    "ADD_BASE_COMPUTE = False\n",
    "TKPM = 20.\n",
    "MIN_FLOPS_SCALE_FACTOR = 10\n",
    "\n",
    "warmstarting_tkpms_df = warmstarting_df[warmstarting_df['tkpm']==TKPM]\n",
    "\n",
    "target_models = sorted(warmstarting_tkpms_df['target_N'].unique())\n",
    "target_models = target_models[1:-1]  # skip the smallest model\n",
    "fig, axes = plt.subplots(1, len(target_models), figsize=(5 * len(target_models), 5), layout='constrained');\n",
    "for i, target_model in enumerate(target_models):\n",
    "    target_model_df = warmstarting_tkpms_df[warmstarting_tkpms_df['target_N']==target_model]\n",
    "    no_growth_df = target_model_df[target_model_df['method']=='mup']\n",
    "    \n",
    "    # calculate flop intervals\n",
    "    max_flops = no_growth_df['flops'].max()\n",
    "    min_flops = max_flops / MIN_FLOPS_SCALE_FACTOR\n",
    "    flop_intervals = np.linspace(min_flops, max_flops, 7)\n",
    "    flops_df = pd.DataFrame()\n",
    "    \n",
    "    # add growth factor == 1\n",
    "    no_growth_df = no_growth_df[no_growth_df[\"base_N\"]==no_growth_df[\"base_N\"].max()]\n",
    "    flops_df[1.] = get_loss_at_flops(no_growth_df, flop_intervals, y_col, unique_col_list)\n",
    "    \n",
    "    # add growth factor > 1\n",
    "    shrink_target_model_df = target_model_df[target_model_df['shrink'] == SHRINK]\n",
    "    shrink_target_model_df = target_model_df[target_model_df['method'] == 'warmstart']\n",
    "    # check if it is in a shrink list\n",
    "    \n",
    "    base_models = sorted(shrink_target_model_df['base_N'].unique(), reverse=True)\n",
    "    for base_model in base_models:\n",
    "        base_model_df = shrink_target_model_df[shrink_target_model_df['base_N']==base_model]\n",
    "        \n",
    "        growth_factor = target_model / base_model\n",
    "        growth_df = base_model_df[base_model_df['target_N']==target_model]\n",
    "        flops_df[growth_factor] = get_loss_at_flops(growth_df, flop_intervals, y_col, unique_col_list, add_base_compute=ADD_BASE_COMPUTE)\n",
    "        # select only the shrink factor we want\n",
    "    axes[i].set_title(f\"Target N: {(target_model/1_000_000):3.1f}M\")\n",
    "    plot_isoflops(\n",
    "        axes[i],\n",
    "        flops_df,\n",
    "        disable_y_label=(i == len(target_models) - 1),\n",
    "    )\n",
    "    \n",
    "# add figure wide xlabel\n",
    "fig.supxlabel(\"Growth Factor\", fontsize=15)\n",
    "fig.supylabel(y_col, fontsize=15)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d24bc1ca2f8a696",
   "metadata": {},
   "source": [
    "SHRINK = 0.4\n",
    "ADD_BASE_COMPUTE = False\n",
    "TKPM = 30.\n",
    "MIN_FLOPS_SCALE_FACTOR = 10\n",
    "\n",
    "warmstarting_tkpms_df = warmstarting_df[warmstarting_df['tkpm']==TKPM]\n",
    "\n",
    "target_models = sorted(warmstarting_tkpms_df['target_N'].unique())\n",
    "target_models = target_models[1:-1]  # skip the smallest model\n",
    "fig, axes = plt.subplots(1, len(target_models), figsize=(5 * len(target_models), 5), layout='constrained');\n",
    "for i, target_model in enumerate(target_models):\n",
    "    target_model_df = warmstarting_tkpms_df[warmstarting_tkpms_df['target_N']==target_model]\n",
    "    no_growth_df = target_model_df[target_model_df['method']=='mup']\n",
    "    \n",
    "    # calculate flop intervals\n",
    "    max_flops = no_growth_df['flops'].max()\n",
    "    min_flops = max_flops / MIN_FLOPS_SCALE_FACTOR\n",
    "    flop_intervals = np.linspace(min_flops, max_flops, 7)\n",
    "    flops_df = pd.DataFrame()\n",
    "    \n",
    "    # add growth factor == 1\n",
    "    no_growth_df = no_growth_df[no_growth_df[\"base_N\"]==no_growth_df[\"base_N\"].max()]\n",
    "    flops_df[1.] = get_loss_at_flops(no_growth_df, flop_intervals, y_col, unique_col_list)\n",
    "    \n",
    "    # add growth factor > 1\n",
    "    shrink_target_model_df = target_model_df[target_model_df['shrink'] == SHRINK]\n",
    "    shrink_target_model_df = target_model_df[target_model_df['method'] == 'warmstart']\n",
    "    # check if it is in a shrink list\n",
    "    \n",
    "    base_models = sorted(shrink_target_model_df['base_N'].unique(), reverse=True)\n",
    "    for base_model in base_models:\n",
    "        base_model_df = shrink_target_model_df[shrink_target_model_df['base_N']==base_model]\n",
    "        \n",
    "        growth_factor = target_model / base_model\n",
    "        growth_df = base_model_df[base_model_df['target_N']==target_model]\n",
    "        flops_df[growth_factor] = get_loss_at_flops(growth_df, flop_intervals, y_col, unique_col_list, add_base_compute=ADD_BASE_COMPUTE)\n",
    "        # select only the shrink factor we want\n",
    "    axes[i].set_title(f\"Target N: {(target_model/1_000_000):3.1f}M\")\n",
    "    plot_isoflops(\n",
    "        axes[i],\n",
    "        flops_df,\n",
    "        disable_y_label=(i == len(target_models) - 1),\n",
    "    )\n",
    "    \n",
    "# add figure wide xlabel\n",
    "fig.supxlabel(\"Growth Factor\", fontsize=15)\n",
    "fig.supylabel(y_col, fontsize=15)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4625165df5ed3169",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
