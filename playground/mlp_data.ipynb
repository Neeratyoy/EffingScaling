{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from itertools import product\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "from scaling.utils import (\n",
    "    get_pareto_frontier, \n",
    "    get_final_points_from_curve_set, \n",
    "    fit_linear_model,\n",
    "    functional_form_chin3,\n",
    "    fit_parametric_form,\n",
    "    fit_parametric_form_stable,\n",
    "    functional_form_chin3_stable,\n",
    ")\n",
    "from scaling.visualize import visualize_train_curves, plot_line_fit, plot_isoflops"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4eda9dc7d708f54",
   "metadata": {},
   "source": [
    "mlp_df = pd.read_parquet(\n",
    "    \"../data/mlp_results.parquet\",\n",
    ")\n",
    "# warmstarting_df = preprocess_warmstarting(warmstarting_df)\n",
    "display(mlp_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aec99c35c2c404f9",
   "metadata": {},
   "source": "mlp_df['method'].unique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6da3451d9b72b65",
   "metadata": {},
   "source": [
    "width_scale_to_params = {\n",
    "    0: 0.1e6,\n",
    "    1: 0.1e6,\n",
    "    2: 0.2e6,\n",
    "    3: 0.4e6,\n",
    "    4: 0.9e6,\n",
    "    5: 1.8e6,\n",
    "    6: 3.5e6,\n",
    "    7: 7.0e6,\n",
    "    8: 14.2e6,\n",
    "    9: 25.2e6,\n",
    "    10: 100.7e6,\n",
    "}\n",
    "width_scale_to_hidden_dim = {\n",
    "    0: 48,\n",
    "    1: 68,\n",
    "    2: 96,\n",
    "    3: 136,\n",
    "    4: 192,\n",
    "    5: 272,\n",
    "    6: 384,\n",
    "    7: 540,\n",
    "    8: 768,\n",
    "    9: 1024,\n",
    "    10: 2048,\n",
    "}\n",
    "\n",
    "# create base_N and target_N columns\n",
    "mlp_df['base_N'] = mlp_df['base_scale'].map(lambda x: width_scale_to_hidden_dim.get(x, np.nan))\n",
    "mlp_df['target_N'] = mlp_df['target_scale'].map(lambda x: width_scale_to_hidden_dim.get(x, np.nan))\n",
    "mlp_df['max_flops'] = mlp_df['curve_flops'].map(lambda x: x[-1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f78935a966db129a",
   "metadata": {},
   "source": [
    "def get_loss_at_flops(df: pd.DataFrame, flop_intervals: list[float]) -> pd.Series:\n",
    "    \"\"\"Get the loss at a specific flop value by interpolation.\"\"\"\n",
    "    best_learning_curve = None\n",
    "    best_final_loss = float('inf')\n",
    "    \n",
    "    # iterate over rows\n",
    "    for row in df.itertuples(index=False):\n",
    "        final_loss = row.curve_train[-1]\n",
    "        if final_loss < best_final_loss:\n",
    "            best_final_loss = final_loss\n",
    "            best_learning_curve = pd.Series(\n",
    "                data=row.curve_val,\n",
    "                index=row.curve_flops\n",
    "            )\n",
    "    \n",
    "    # add the flops into the Series if not present\n",
    "    for flop in flop_intervals:\n",
    "        if flop not in best_learning_curve.index:\n",
    "            best_learning_curve.loc[flop] = np.nan\n",
    "    best_learning_curve = best_learning_curve.sort_index()\n",
    "    # interpolate nans\n",
    "    best_learning_curve = best_learning_curve.interpolate(method='linear')\n",
    "    return best_learning_curve.loc[flop_intervals]\n",
    "\n",
    "\n",
    "\n",
    "TKPM = 50.\n",
    "MIN_FLOPS_SCALE_FACTOR = 8\n",
    "SCALING = \"width\"\n",
    "METHOD = \"net2net\"\n",
    "\n",
    "\n",
    "mlp_selection_df = mlp_df[mlp_df['tkpm_group']==TKPM]\n",
    "mlp_selection_df = mlp_selection_df[mlp_selection_df['scaling']==SCALING]\n",
    "\n",
    "target_models = sorted(mlp_selection_df['target_N'].unique())[-4:]# [-3:-1]\n",
    "fig, axes = plt.subplots(1, len(target_models), figsize=(5 * len(target_models), 5), layout='constrained');\n",
    "axes = np.atleast_1d(axes)\n",
    "for i, target_model in enumerate(target_models):\n",
    "    target_model_df = mlp_selection_df[mlp_selection_df['target_N']==target_model]\n",
    "    no_growth_df = target_model_df[target_model_df['method']=='scratch']\n",
    "    \n",
    "    # calculate flop intervals\n",
    "    max_flops = no_growth_df['max_flops'].max()\n",
    "    min_flops = max_flops / MIN_FLOPS_SCALE_FACTOR\n",
    "    flop_intervals = np.linspace(min_flops, max_flops, 7)\n",
    "    flops_df = pd.DataFrame()\n",
    "    \n",
    "    # add growth factor == 1\n",
    "    flops_df[1.] = get_loss_at_flops(no_growth_df, flop_intervals)\n",
    "    \n",
    "    # add growth factor > 1\n",
    "    shrink_target_model_df = target_model_df[target_model_df['method'] == METHOD]\n",
    "    # check if it is in a shrink list\n",
    "    \n",
    "    base_models = sorted(shrink_target_model_df['base_N'].unique(), reverse=True)\n",
    "    for base_model in base_models:\n",
    "        base_model_df = shrink_target_model_df[shrink_target_model_df['base_N']==base_model]\n",
    "        \n",
    "        growth_factor = target_model / base_model\n",
    "        growth_df = base_model_df[base_model_df['target_N']==target_model]\n",
    "        flops_df[growth_factor] = get_loss_at_flops(growth_df, flop_intervals)\n",
    "        # select only the shrink factor we want\n",
    "    # axes[i].set_title(f\"Target N: {(target_model/1_000_000):3.1f}M\")\n",
    "    axes[i].set_title(f\"Width {target_model}\")\n",
    "    plot_isoflops(\n",
    "        axes[i],\n",
    "        flops_df,\n",
    "        disable_y_label=(i == len(target_models) - 1),\n",
    "    )\n",
    "    \n",
    "# add figure wide xlabel\n",
    "fig.suptitle(f\"{METHOD} - {TKPM} tkpm\")\n",
    "fig.supxlabel(\"Growth Factor\", fontsize=15)\n",
    "fig.supylabel(\"Validation Loss\", fontsize=15)\n",
    "plt.show()\n",
    "fig.savefig(f\"figures/mlp_isoflops_{TKPM}_tkpm.pdf\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot Rank Correlation\n",
   "id": "f589341234105cf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "\n",
    "def load_and_process_hpo_results(df, x_axis = \"percent\", correlation_type='spearmanr') -> pd.DataFrame:\n",
    "    assert x_axis in ['flops', 'percent']\n",
    "    df = df.copy()\n",
    "    if correlation_type == 'spearmanr':\n",
    "        correlation_function = spearmanr\n",
    "    elif correlation_type == \"kendalltau\":\n",
    "        correlation_function = kendalltau\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown correlation: {correlation_type}\")\n",
    "    \n",
    "    \n",
    "    df['max_flops'] = df['curve_flops'].map(lambda x: np.nanmax(x))\n",
    "    df['min_flops'] = df['curve_flops'].map(lambda x: np.nanmin(x))\n",
    "    df['curve_percent'] = df['curve_flops'].map(lambda x: x / x.max() * 100)\n",
    "    \n",
    "    if x_axis == 'flops':\n",
    "        intervals = np.linspace(df['min_flops'].max(), df['max_flops'].min(), 200)\n",
    "    elif x_axis == 'percent':\n",
    "        intervals = np.linspace(0, 100, 200)[1:]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown x_axis: {x_axis}\")\n",
    "    \n",
    "    df['final_val_loss'] = df['curve_val'].map(lambda x: [x[-1],]*len(intervals))\n",
    "    \n",
    "    values = []\n",
    "    target_values = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        _df = pd.DataFrame({\n",
    "            'val': row.curve_val,\n",
    "        }, index=row.curve_percent if x_axis == \"percent\" else row.curve_flops).dropna()\n",
    "        _df = _df.reindex(_df.index.union(intervals)).sort_index()\n",
    "        _df['val'] = _df['val'].interpolate(method='linear')\n",
    "        _df = _df.loc[intervals]\n",
    "        values.append(_df['val'].values)\n",
    "        target_values.append(row.final_val_loss)\n",
    "    values = np.array(values)\n",
    "    target_values = np.array(target_values)\n",
    "    \n",
    "    # apply correlation function column-wise\n",
    "    correlations = []\n",
    "    for i in range(values.shape[1]):\n",
    "        correlations.append(correlation_function(values[:, i], target_values[:, i]).correlation)\n",
    "    return pd.DataFrame({\n",
    "        \"intervals\": intervals,\n",
    "        \"curve_correlation\": correlations,\n",
    "    })\n",
    "\n",
    "hyperparameter_columns = ['cfg_warmup_fraction', 'cfg_cooldown_fraction']\n",
    "\n",
    "TKPM = 20.\n",
    "MIN_FLOPS_SCALE_FACTOR = 8\n",
    "SCALING = \"width\"\n",
    "METHOD = \"snp\"\n",
    "mlp_selection_df = mlp_df[mlp_df['tkpm_group']==TKPM]\n",
    "mlp_selection_df = mlp_selection_df[mlp_selection_df['scaling']==SCALING]\n",
    "mlp_selection_df_method = mlp_selection_df[mlp_selection_df['method']==METHOD]\n",
    "mlp_selection_df_mup = mlp_selection_df[mlp_selection_df['method']=='mup']\n",
    "\n",
    "base_scales = sorted(mlp_selection_df_method['base_scale'].unique())\n",
    "target_scales = sorted(mlp_selection_df_method['target_scale'].unique())\n",
    "fig, axes = plt.subplots(len(base_scales), len(target_scales), figsize=(5 * len(target_models), 4.5 * len(base_scales) + 0.5), layout='constrained');\n",
    "\n",
    "for i, base_scale in enumerate(base_scales):\n",
    "    for j, target_scale in enumerate(target_scales):\n",
    "        method_df = mlp_selection_df_method[\n",
    "            (mlp_selection_df_method['base_scale']==base_scale) &\n",
    "            (mlp_selection_df_method['target_scale']==target_scale)\n",
    "        ]\n",
    "        if len(method_df) != 6:\n",
    "            continue\n",
    "        print(f\"Base: {base_scale}, Target: {target_scale}, Num Configs: {len(method_df)}\")\n",
    "        hpo_df = load_and_process_hpo_results(method_df)\n",
    "\n",
    "        axes[i,j].plot(hpo_df['intervals'], hpo_df['curve_correlation'], label=METHOD, color='blue')\n",
    "        axes[i,j].axhline(y=1, color='black', linestyle=':', alpha=.4)\n",
    "        \n",
    "        mup_df = mlp_selection_df_mup[\n",
    "            (mlp_selection_df_mup['base_scale']==base_scale) &\n",
    "            (mlp_selection_df_mup['target_scale']==target_scale)\n",
    "        ]\n",
    "        if len(mup_df) == 6:\n",
    "            hpo_mup_df = load_and_process_hpo_results(mup_df)\n",
    "            axes[i,j].plot(hpo_mup_df['intervals'], hpo_mup_df['curve_correlation'], label='mup', color='red', linestyle='--')\n",
    "        \n",
    "        axes[i,j].set_title(f\"Base: {base_scale}, Target: {target_scale}\")\n",
    "        axes[i,j].legend()\n",
    "        "
   ],
   "id": "ed827a725ff7d07c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "186caed9581390f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hpo_mup_df",
   "id": "f094b6159cc56a38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mup_df",
   "id": "1b44efb30019cd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c735c1af2f385d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
